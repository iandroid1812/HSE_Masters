{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_funcs.data import cleaned_market, load_market\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import ta\n",
    "import pickle\n",
    "from darts import TimeSeries\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "import numpy as np\n",
    "from darts.utils.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_init(time_col, static_cols,\\\n",
    "    value_cols, freq, fill_missing, type, ticker=None, group_col=None, EXP_MA=14):\n",
    "\n",
    "    if type=='MULTI':\n",
    "        data = cleaned_market()\n",
    "    elif type=='UNI':\n",
    "        data = cleaned_market()\n",
    "    elif type=='Sentiment':\n",
    "        try:\n",
    "            data = pd.read_pickle('Datasets/market/market_sentiment.pkl')\n",
    "        except:\n",
    "            timeseries = timeseries_init(\n",
    "                time_col='Date',\n",
    "                static_cols=[],\n",
    "                value_cols=[\n",
    "                    'Adj Close',\n",
    "                    'Close',\n",
    "                    'High',\n",
    "                    'Low',\n",
    "                    'Open',\n",
    "                    'Volume'],\n",
    "                freq=freq,\n",
    "                fill_missing=True,\n",
    "                group_col='Ticker',\n",
    "                type='MULTI'\n",
    "                )\n",
    " \n",
    "            dictionary = sentiment_init(timeseries[0], EXP_MA)\n",
    "\n",
    "            data = pd.DataFrame([])\n",
    "            for i in range(6):\n",
    "                df = timeseries[i].pd_dataframe().reset_index()\n",
    "                df['Ticker'] = timeseries[i].static_covariates_values()[0][0]\n",
    "                data = pd.concat([data, df], axis=0)\n",
    "            data = data.sort_values(by=['Date', 'Ticker']).reset_index(drop=True)\n",
    "            data.Date = data.Date.dt.date.apply(lambda x: str(x))\n",
    "\n",
    "            data['sentiment'] = None\n",
    "            for key in list(dictionary.keys()):\n",
    "                data['sentiment'] = data['sentiment'].combine_first(\n",
    "                    pd.merge(data, dictionary[key], how ='left', on=['Date', 'Ticker']).sentiment_score\n",
    "                    )\n",
    "            data.to_pickle('Datasets/market/market_sentiment.pkl')\n",
    "    \n",
    "\n",
    "    if type=='MULTI':\n",
    "        timeseries = TimeSeries.from_group_dataframe(\n",
    "            df=data,\n",
    "            time_col=time_col,\n",
    "            group_cols=group_col,  # individual time series are extracted by grouping `df` by `group_cols`\n",
    "            static_cols=static_cols,\n",
    "            value_cols=value_cols,\n",
    "            freq=freq,\n",
    "            fill_missing_dates=fill_missing\n",
    "            )\n",
    "\n",
    "        for i in range(len(timeseries)):\n",
    "            timeseries[i] = fill_missing_values(timeseries[i], method='ffill', limit_direction='forward')\n",
    "            timeseries[i] = timeseries[i].add_holidays(country_code='US')\n",
    "\n",
    "        \n",
    "\n",
    "    elif type=='Sentiment':\n",
    "        timeseries = TimeSeries.from_group_dataframe(\n",
    "            df=data,\n",
    "            time_col=time_col,\n",
    "            group_cols=group_col,  # individual time series are extracted by grouping `df` by `group_cols`\n",
    "            static_cols=static_cols,\n",
    "            value_cols=value_cols,\n",
    "            freq=freq\n",
    "            )\n",
    "\n",
    "        for i in range(len(timeseries)):\n",
    "            # timeseries[i] = fill_missing_values(timeseries[i], method='ffill', limit_direction='forward')\n",
    "            timeseries[i] = timeseries[i].add_holidays(country_code='US')\n",
    "    \n",
    "    else:\n",
    "        timeseries = None\n",
    "\n",
    "\n",
    "    return timeseries\n",
    "\n",
    "\n",
    "def get_covariates(type, data, target_col, past_cov, future_cov):\n",
    "\n",
    "    if type=='MULTI':\n",
    "        target_train, past_train, future_train,\\\n",
    "            target_val, past_val, future_val = [[] for _ in range(6)]\n",
    "\n",
    "\n",
    "        for series in data['train']:\n",
    "            target_train.append(series[target_col])\n",
    "            past_train.append(series[past_cov])\n",
    "            future_train.append(series[future_cov])\n",
    "\n",
    "        for series in data['val']:\n",
    "            target_val.append(series[target_col])\n",
    "            past_val.append(series[past_cov])\n",
    "            future_val.append(series[future_cov])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "    return target_train, past_train, future_train,\\\n",
    "        target_val, past_val, future_val\n",
    "\n",
    "def frac(neg, pos):\n",
    "    if neg == 0:\n",
    "        return 1\n",
    "    if pos == 0:\n",
    "        return 0\n",
    "    return neg / pos\n",
    "\n",
    "def sentiment_init(timeseries, EXP_MA):\n",
    "    df = pd.read_pickle(\"Datasets/results/preprocessing/sentiment_finetuned.pkl\")\n",
    "    tickers_df = pd.read_csv(\"Datasets/kaggle/Company_Tweet.csv\")\n",
    "\n",
    "    df = df.merge(tickers_df, on='tweet_id', how='inner')\n",
    "\n",
    "    df = df[['post_date', 'sentiment_score', 'ticker_symbol']]\n",
    "\n",
    "    df_positive = df[df.sentiment_score==1]\n",
    "    df_positive['post_date'] = df_positive['post_date'].apply(lambda x: x.to_pydatetime().date())\n",
    "    df_positive = df_positive.groupby(['ticker_symbol', 'post_date']).count()\n",
    "\n",
    "    df_negative = df[df.sentiment_score==0]\n",
    "    df_negative['post_date'] = df_negative['post_date'].apply(lambda x: x.to_pydatetime().date())\n",
    "    df_negative = df_negative.groupby(['ticker_symbol', 'post_date']).count()\n",
    "\n",
    "    business_days = pd.DataFrame(\n",
    "        {'Date': timeseries.time_index.to_frame().reset_index(drop=True)['Date'].dt.date})\n",
    "\n",
    "    sentiment_dict = {}\n",
    "    tickers = ['AAPL', 'GOOG', 'GOOGL', 'AMZN', 'MSFT', 'TSLA']\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        negative = df_negative.xs(ticker).reindex(pd.date_range('2015-01-01', '2019-12-31'), fill_value=0)\n",
    "        negative.index = negative.index.date\n",
    "        positive = df_positive.xs(ticker).reindex(pd.date_range('2015-01-01', '2019-12-31'), fill_value=0)\n",
    "        positive.index = positive.index.date\n",
    "\n",
    "        lst_negative = []\n",
    "        i = 0\n",
    "        for date_b in business_days.Date:\n",
    "            sum = 0\n",
    "            for date in negative.index[i:]:\n",
    "                i += 1\n",
    "                sum += negative.loc[date].sentiment_score\n",
    "                if date_b == date:\n",
    "                    lst_negative.append(sum)\n",
    "                    break\n",
    "\n",
    "        lst_positive = []\n",
    "        i = 0\n",
    "        for date_b in business_days.Date:\n",
    "            sum = 0\n",
    "            for date in positive.index[i:]:\n",
    "                i += 1\n",
    "                sum += positive.loc[date].sentiment_score\n",
    "                if date_b == date:\n",
    "                    lst_positive.append(sum)\n",
    "                    break\n",
    "            \n",
    "        new_df = pd.concat(\n",
    "            [business_days.Date, pd.Series(lst_negative), pd.Series(lst_positive)],\n",
    "            axis=1\n",
    "            ).rename(columns = {0:'Negative', 1:'Positive'})\n",
    "\n",
    "        sentiment_score = pd.DataFrame(\n",
    "            {\n",
    "                'sentiment_score': new_df.apply(lambda row: frac(row['Negative'], row['Positive']), axis=1),\n",
    "                'Date': business_days.Date.apply(lambda x: str(x))\n",
    "                }\n",
    "                )\n",
    "        \n",
    "        sentiment_score.sentiment_score = sentiment_score.sentiment_score.ewm(EXP_MA).mean()\n",
    "        \n",
    "        sentiment_score['Ticker'] = ticker\n",
    "        \n",
    "        sentiment_dict[ticker] = sentiment_score\n",
    "    \n",
    "    return sentiment_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of negative tweets in all tweets\n",
    "def frac_by_all(negative, positive):\n",
    "    if negative == 0:\n",
    "        return 0\n",
    "    return negative / (negative + positive)\n",
    "\n",
    "def frac(negative, positive):\n",
    "    if negative == 0:\n",
    "        return 0\n",
    "    elif positive == 0:\n",
    "        return 1\n",
    "    return negative / positive\n",
    "\n",
    "\n",
    "def market_init(freq='B', EXP_MA=15):\n",
    "    data = load_market()\n",
    "    tickers_df = pd.read_csv(\"Datasets/kaggle/Company_Tweet.csv\")\n",
    "    tickers = tickers_df.ticker_symbol.value_counts().index.to_list()\n",
    "    tickers.sort()\n",
    "\n",
    "\n",
    "    data['Volatility'] = ta.volatility.AverageTrueRange(\n",
    "        high=data.High, low=data.Low, close=data.Close, window=EXP_MA).average_true_range()\n",
    "\n",
    "    data = data[data.Date > '2014.12.31'].reset_index(drop=True)\n",
    "\n",
    "    for ticker in tickers:\n",
    "        df = data[data.Ticker == ticker]\n",
    "        df = df.set_index('Date').resample(freq).ffill().reset_index()\n",
    "    \n",
    "        if ticker == 'AAPL':\n",
    "            market_df = df\n",
    "        else:\n",
    "            market_df = pd.concat([market_df, df])\n",
    "    \n",
    "    market_df = market_df.sort_values(by=['Date', 'Ticker']).reset_index(drop=True)\n",
    "\n",
    "    return market_df\n",
    "\n",
    "\n",
    "def sentiment_init(freq='B', EXP_MA=15):\n",
    "    tickers_df = pd.read_csv(\"Datasets/kaggle/Company_Tweet.csv\")\n",
    "    df_sentiment = pd.read_pickle(\"Datasets/results/preprocessing/sentiment_finetuned.pkl\")\n",
    "    tickers = tickers_df.ticker_symbol.value_counts().index.to_list()\n",
    "    tickers.sort()\n",
    "\n",
    "    df_sentiment = df_sentiment.merge(tickers_df, on='tweet_id', how='inner')\n",
    "    df_sentiment = df_sentiment[['post_date', 'sentiment_score', 'ticker_symbol']]\n",
    "    df_sentiment.loc[:, 'post_date'] = df_sentiment.post_date.apply(lambda x: x.to_pydatetime().date())\n",
    "    df_sentiment = df_sentiment.groupby(['ticker_symbol', 'post_date']).value_counts().unstack(fill_value=0)\n",
    "\n",
    "    for ticker in tickers:\n",
    "        # first we add missing dates and fill thew with 0\n",
    "        company = df_sentiment.xs(ticker).reindex(pd.date_range('2015-01-02', '2019-12-31'), fill_value=0)\n",
    "\n",
    "        # then resample to business days and add up dropped past values up to the date that is not dropped\n",
    "        company = company.resample(rule=freq, origin='end').sum().rename(columns = {0:'Negative', 1:'Positive'})\n",
    "\n",
    "        # taking fractions of negative tweeets to calculate the final score \n",
    "        company['sentiment_score_1'] = company.apply(lambda row: frac_by_all(row['Negative'], row['Positive']), axis=1)\n",
    "        company['sentiment_score_2'] = company.apply(lambda row: frac(row['Negative'], row['Positive']), axis=1)\n",
    "\n",
    "        # calculating moving average\n",
    "        company['sentiment_score_1'] = company.sentiment_score_1.ewm(span=EXP_MA).mean()\n",
    "        company['sentiment_score_2'] = company.sentiment_score_2.ewm(span=EXP_MA).mean()\n",
    "\n",
    "        company['Ticker'] = ticker\n",
    "        company = company.reset_index().rename(\n",
    "            columns={\n",
    "                'index': 'Date'\n",
    "                })\n",
    "        \n",
    "        if ticker == 'AAPL':\n",
    "            sentiment_df = company\n",
    "        else:\n",
    "            sentiment_df = pd.concat([sentiment_df, company])\n",
    "    \n",
    "    sentiment_df = sentiment_df.sort_values(by=['Date', 'Ticker']).reset_index(drop=True)\n",
    "    sentiment_df.columns.name = None\n",
    "\n",
    "    return sentiment_df\n",
    "\n",
    "\n",
    "def embeddings_init(freq='B'):\n",
    "    with open('Datasets/results/preprocessing/embeddings_2.pkl', \"rb\") as file:\n",
    "        embeddings = pickle.load(file)\n",
    "        file.close()\n",
    "    tickers_df = pd.read_csv(\"Datasets/kaggle/Company_Tweet.csv\")\n",
    "    tickers = tickers_df.ticker_symbol.value_counts().index.to_list()\n",
    "    tickers.sort()\n",
    "\n",
    "    df = pd.read_pickle(\"Datasets/results/preprocessing/final.pkl\")\n",
    "    df = df[['tweet_id', 'post_date']]\n",
    "    df = df.merge(tickers_df, on='tweet_id', how='inner').rename(\n",
    "        columns={\n",
    "            'post_date': 'Date',\n",
    "            'ticker_symbol': 'Ticker'\n",
    "        })\n",
    "\n",
    "    df['Date'] = df['Date'].apply(lambda x: x.date())\n",
    "    df = df.reset_index()\n",
    "    df = df.groupby(['Ticker', 'Date']).agg({'index': lambda x: embeddings[list(x)].mean(axis=0).tolist()})\n",
    "    df = df.rename(columns={\n",
    "        'index': 'embeddings'\n",
    "    })\n",
    "\n",
    "    df.embeddings = df.embeddings.apply(np.array)\n",
    "\n",
    "    for ticker in tickers:\n",
    "        # first we add missing dates and fill thew with 0\n",
    "        company = df.xs(ticker).reindex(pd.date_range('2015-01-02', '2019-12-31'), fill_value=np.zeros(384))\n",
    "\n",
    "        # then resample to business days and add up dropped past values up to the date that is not dropped\n",
    "        company = company.resample(rule=freq, origin='end').mean()\n",
    "\n",
    "        company['Ticker'] = ticker\n",
    "        \n",
    "        if ticker == 'AAPL':\n",
    "            embeddings_df = company\n",
    "        else:\n",
    "            embeddings_df = pd.concat([embeddings_df, company])\n",
    "    \n",
    "    embeddings_df = embeddings_df.reset_index().rename(\n",
    "            columns={\n",
    "                'index': 'Date'\n",
    "                })\n",
    "    embeddings_df = embeddings_df.sort_values(by=['Date', 'Ticker']).reset_index(drop=True)\n",
    "    return embeddings_df\n",
    "\n",
    "\n",
    "def total_init():\n",
    "    if exists('Datasets/results/total_df.pkl'):\n",
    "        return pd.read_pickle('Datasets/results/total_df.pkl')\n",
    "\n",
    "    market = market_init()\n",
    "    sentiment = sentiment_init()\n",
    "    embeddings = embeddings_init()\n",
    "\n",
    "    total = pd.merge(market, sentiment, on=['Date', 'Ticker'])\n",
    "    total = pd.merge(total, embeddings, on=['Date', 'Ticker'])\n",
    "    total= total.join(pd.DataFrame(total['embeddings'].to_list()))\n",
    "\n",
    "    total.to_pickle('Datasets/results/total_df.pkl')\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "def total_timeseries(market=True, sentiment=True, embeddings=True):\n",
    "\n",
    "    market_columns = []\n",
    "    if market is True:\n",
    "        market_columns = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume', 'Volatility']\n",
    "\n",
    "    sentiment_columns = []\n",
    "    if sentiment is True:\n",
    "        sentiment_columns = ['Negative', 'Positive', 'sentiment_score_1', 'sentiment_score_2']\n",
    "    \n",
    "    embeddings_columns = []\n",
    "    if embeddings is True:\n",
    "        embeddings_columns = np.arange(384).tolist()\n",
    "\n",
    "    value_columns = market_columns + sentiment_columns + embeddings_columns\n",
    "    \n",
    "    data = total_init()\n",
    "\n",
    "    timeseries_total = TimeSeries.from_group_dataframe(\n",
    "        df=data,\n",
    "        time_col='Date',\n",
    "        group_cols='Ticker',\n",
    "        static_cols=[],\n",
    "        value_cols=value_columns,\n",
    "        freq='B'\n",
    "    )\n",
    "\n",
    "    for i in range(len(timeseries_total)):\n",
    "        timeseries_total[i] = timeseries_total[i].add_holidays(country_code='US')\n",
    "\n",
    "    return timeseries_total\n",
    "\n",
    "\n",
    "def get_covariates(data, past_covariates, embeddings=False):\n",
    "    train = data[0]\n",
    "    val = data[1]\n",
    "    emb = np.arange(384).tolist()\n",
    "    emb = list(map(str, emb))\n",
    "\n",
    "    if embeddings:\n",
    "       past_covariates = past_covariates + emb\n",
    "\n",
    "    target_train, past_train, future_train,\\\n",
    "        target_val, past_val, future_val = [[] for _ in range(6)]\n",
    "\n",
    "    for series_1, series_2 in zip(train, val):\n",
    "        target_train.append(series_1['Close'])\n",
    "        past_train.append(series_1[past_covariates])\n",
    "        future_train.append(series_1['holidays'])\n",
    "\n",
    "        target_val.append(series_2['Close'])\n",
    "        past_val.append(series_2[past_covariates])\n",
    "        future_val.append(series_2['holidays'])\n",
    "\n",
    "    return target_train, past_train, future_train,\\\n",
    "        target_val, past_val, future_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Negative</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>24.603205</td>\n",
       "      <td>27.332500</td>\n",
       "      <td>27.860001</td>\n",
       "      <td>26.837500</td>\n",
       "      <td>27.847500</td>\n",
       "      <td>212818400</td>\n",
       "      <td>15.702441</td>\n",
       "      <td>227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019345</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>-0.009246</td>\n",
       "      <td>-0.011367</td>\n",
       "      <td>-0.029173</td>\n",
       "      <td>0.010009</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>-0.060718</td>\n",
       "      <td>-0.017831</td>\n",
       "      <td>0.031880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>15.426000</td>\n",
       "      <td>15.426000</td>\n",
       "      <td>15.737500</td>\n",
       "      <td>15.348000</td>\n",
       "      <td>15.629000</td>\n",
       "      <td>55664000</td>\n",
       "      <td>15.454578</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034497</td>\n",
       "      <td>0.009232</td>\n",
       "      <td>-0.017027</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>-0.030030</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.019768</td>\n",
       "      <td>-0.060930</td>\n",
       "      <td>-0.028438</td>\n",
       "      <td>0.025051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>26.168653</td>\n",
       "      <td>26.168653</td>\n",
       "      <td>26.490770</td>\n",
       "      <td>26.133251</td>\n",
       "      <td>26.378078</td>\n",
       "      <td>28951268</td>\n",
       "      <td>15.161924</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040227</td>\n",
       "      <td>-0.012218</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>-0.013846</td>\n",
       "      <td>-0.034779</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.028838</td>\n",
       "      <td>-0.044456</td>\n",
       "      <td>-0.006530</td>\n",
       "      <td>0.022659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>26.477501</td>\n",
       "      <td>26.477501</td>\n",
       "      <td>26.790001</td>\n",
       "      <td>26.393999</td>\n",
       "      <td>26.629999</td>\n",
       "      <td>26480000</td>\n",
       "      <td>14.192552</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038495</td>\n",
       "      <td>0.017129</td>\n",
       "      <td>-0.010664</td>\n",
       "      <td>-0.010565</td>\n",
       "      <td>-0.037532</td>\n",
       "      <td>0.014501</td>\n",
       "      <td>0.028705</td>\n",
       "      <td>-0.053237</td>\n",
       "      <td>-0.022100</td>\n",
       "      <td>0.029239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>40.811424</td>\n",
       "      <td>46.759998</td>\n",
       "      <td>47.419998</td>\n",
       "      <td>46.540001</td>\n",
       "      <td>46.660000</td>\n",
       "      <td>27913900</td>\n",
       "      <td>14.642549</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019906</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>-0.016480</td>\n",
       "      <td>-0.010017</td>\n",
       "      <td>0.022173</td>\n",
       "      <td>0.016566</td>\n",
       "      <td>-0.054880</td>\n",
       "      <td>-0.004975</td>\n",
       "      <td>0.011813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7813</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>92.391998</td>\n",
       "      <td>92.391998</td>\n",
       "      <td>92.663002</td>\n",
       "      <td>91.611504</td>\n",
       "      <td>92.099998</td>\n",
       "      <td>50130000</td>\n",
       "      <td>52.694159</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>-0.021011</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>-0.025918</td>\n",
       "      <td>0.008945</td>\n",
       "      <td>0.015516</td>\n",
       "      <td>-0.062164</td>\n",
       "      <td>-0.035174</td>\n",
       "      <td>0.032179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7814</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>66.850998</td>\n",
       "      <td>66.850998</td>\n",
       "      <td>66.900002</td>\n",
       "      <td>66.454247</td>\n",
       "      <td>66.505501</td>\n",
       "      <td>19236000</td>\n",
       "      <td>50.910398</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032021</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>-0.024404</td>\n",
       "      <td>-0.004225</td>\n",
       "      <td>-0.025769</td>\n",
       "      <td>0.015236</td>\n",
       "      <td>0.058303</td>\n",
       "      <td>-0.037929</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.031112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7815</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>66.969498</td>\n",
       "      <td>66.969498</td>\n",
       "      <td>67.032997</td>\n",
       "      <td>66.606499</td>\n",
       "      <td>66.789497</td>\n",
       "      <td>19514000</td>\n",
       "      <td>47.544805</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014872</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>-0.027154</td>\n",
       "      <td>-0.012286</td>\n",
       "      <td>-0.023972</td>\n",
       "      <td>0.020188</td>\n",
       "      <td>0.028995</td>\n",
       "      <td>-0.057463</td>\n",
       "      <td>-0.012534</td>\n",
       "      <td>0.012191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7816</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>153.313156</td>\n",
       "      <td>157.699997</td>\n",
       "      <td>157.770004</td>\n",
       "      <td>156.449997</td>\n",
       "      <td>156.770004</td>\n",
       "      <td>18369400</td>\n",
       "      <td>50.428518</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>0.011812</td>\n",
       "      <td>-0.013947</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>-0.019997</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.024076</td>\n",
       "      <td>-0.067404</td>\n",
       "      <td>-0.013164</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7817</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>27.888666</td>\n",
       "      <td>27.888666</td>\n",
       "      <td>28.086000</td>\n",
       "      <td>26.805332</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>154285500</td>\n",
       "      <td>55.792928</td>\n",
       "      <td>335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029754</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>-0.020334</td>\n",
       "      <td>0.005410</td>\n",
       "      <td>-0.025857</td>\n",
       "      <td>-0.003051</td>\n",
       "      <td>0.031505</td>\n",
       "      <td>-0.042624</td>\n",
       "      <td>-0.003159</td>\n",
       "      <td>0.029068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7818 rows Ã— 398 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date Ticker   Adj Close       Close        High         Low  \\\n",
       "0    2015-01-02   AAPL   24.603205   27.332500   27.860001   26.837500   \n",
       "1    2015-01-02   AMZN   15.426000   15.426000   15.737500   15.348000   \n",
       "2    2015-01-02   GOOG   26.168653   26.168653   26.490770   26.133251   \n",
       "3    2015-01-02  GOOGL   26.477501   26.477501   26.790001   26.393999   \n",
       "4    2015-01-02   MSFT   40.811424   46.759998   47.419998   46.540001   \n",
       "...         ...    ...         ...         ...         ...         ...   \n",
       "7813 2019-12-31   AMZN   92.391998   92.391998   92.663002   91.611504   \n",
       "7814 2019-12-31   GOOG   66.850998   66.850998   66.900002   66.454247   \n",
       "7815 2019-12-31  GOOGL   66.969498   66.969498   67.032997   66.606499   \n",
       "7816 2019-12-31   MSFT  153.313156  157.699997  157.770004  156.449997   \n",
       "7817 2019-12-31   TSLA   27.888666   27.888666   28.086000   26.805332   \n",
       "\n",
       "            Open     Volume  Volatility  Negative  ...       374       375  \\\n",
       "0      27.847500  212818400   15.702441       227  ...  0.019345  0.008206   \n",
       "1      15.629000   55664000   15.454578        37  ...  0.034497  0.009232   \n",
       "2      26.378078   28951268   15.161924        16  ...  0.040227 -0.012218   \n",
       "3      26.629999   26480000   14.192552        11  ...  0.038495  0.017129   \n",
       "4      46.660000   27913900   14.642549        15  ...  0.019906  0.000464   \n",
       "...          ...        ...         ...       ...  ...       ...       ...   \n",
       "7813   92.099998   50130000   52.694159        42  ...  0.023598  0.009558   \n",
       "7814   66.505501   19236000   50.910398        12  ...  0.032021  0.003363   \n",
       "7815   66.789497   19514000   47.544805        11  ...  0.014872  0.005204   \n",
       "7816  156.770004   18369400   50.428518        26  ...  0.008740  0.011812   \n",
       "7817   27.000000  154285500   55.792928       335  ...  0.029754  0.000590   \n",
       "\n",
       "           376       377       378       379       380       381       382  \\\n",
       "0    -0.009246 -0.011367 -0.029173  0.010009  0.034422 -0.060718 -0.017831   \n",
       "1    -0.017027 -0.001370 -0.030030  0.007514  0.019768 -0.060930 -0.028438   \n",
       "2     0.004907 -0.013846 -0.034779  0.007376  0.028838 -0.044456 -0.006530   \n",
       "3    -0.010664 -0.010565 -0.037532  0.014501  0.028705 -0.053237 -0.022100   \n",
       "4     0.004415 -0.016480 -0.010017  0.022173  0.016566 -0.054880 -0.004975   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7813 -0.021011  0.001861 -0.025918  0.008945  0.015516 -0.062164 -0.035174   \n",
       "7814 -0.024404 -0.004225 -0.025769  0.015236  0.058303 -0.037929  0.003818   \n",
       "7815 -0.027154 -0.012286 -0.023972  0.020188  0.028995 -0.057463 -0.012534   \n",
       "7816 -0.013947  0.003805 -0.019997  0.009847  0.024076 -0.067404 -0.013164   \n",
       "7817 -0.020334  0.005410 -0.025857 -0.003051  0.031505 -0.042624 -0.003159   \n",
       "\n",
       "           383  \n",
       "0     0.031880  \n",
       "1     0.025051  \n",
       "2     0.022659  \n",
       "3     0.029239  \n",
       "4     0.011813  \n",
       "...        ...  \n",
       "7813  0.032179  \n",
       "7814  0.031112  \n",
       "7815  0.012191  \n",
       "7816  0.028100  \n",
       "7817  0.029068  \n",
       "\n",
       "[7818 rows x 398 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total_timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(\n",
    "    total,\n",
    "    axis=1,\n",
    "    test_size=0.2,\n",
    "    input_size=15,\n",
    "    horizon=5,\n",
    "    vertical_split_type='model-aware'\n",
    ")\n",
    "\n",
    "data = [train, val]\n",
    "\n",
    "target_train, past_train, future_train, target_val, past_val, future_val = get_covariates(\n",
    "    data, \n",
    "    [\n",
    "        'Adj Close',\n",
    "        'High',\n",
    "        'Low',\n",
    "        'Open',\n",
    "        'Volume',\n",
    "        'Volatility',\n",
    "        'Negative',\n",
    "        'Positive',\n",
    "        'sentiment_score_1',\n",
    "        'sentiment_score_2'\n",
    "        ],\n",
    "    embeddings=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02409fb671b1ed46b2b92a72e18ceb6708409a9b6d1d011f25cd31b784aeb6de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
